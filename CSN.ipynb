{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn import svm\n",
    "import random\n",
    "import tarfile\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.manifold import TSNE\n",
    "import encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/data/wikiart/wikiart_Painting100k/MultitaskPainting100k_Dataset_groundtruth/groundtruth_multiloss_train_header.csv')\n",
    "test_df = pd.read_csv('/data/wikiart/wikiart_Painting100k/MultitaskPainting100k_Dataset_groundtruth/groundtruth_multiloss_test_header.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['img_path'] = train_df.apply(lambda x: join('/data/wikiart/wikiart_Painting100k/images_256minside',x.filename),1)\n",
    "test_df['img_path'] = test_df.apply(lambda x: join('/data/wikiart/wikiart_Painting100k/images_256minside',x.filename),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.replace('wildlife painting', 'animal painting')\n",
    "train_df = train_df.replace('self-portrait', 'portrait')\n",
    "train_df = train_df.replace('poster', 'design')\n",
    "train_df = train_df.replace('advertisement', 'illustration')\n",
    "train_df = train_df.replace('cloudscape', 'landscape')\n",
    "train_df = train_df.replace('literary painting', 'mythological painting')\n",
    "train_df = train_df.replace('battle painting', 'history painting')\n",
    "train_df = train_df.replace('bird-and-flower painting', 'animal painting')\n",
    "train_df = train_df[train_df.genre.isin(['shan shui','panorama','miniature','pastorale','quadratura','vanitas','bijinga',\n",
    "                                        'calligraphy','yakusha-e'])==False]\n",
    "#train_df = train_df[train_df.genre.isin(['history painting','allegorical painting','interior','capriccio','veduta','caricature','tessellation'])==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.replace('wildlife painting', 'animal painting')\n",
    "test_df = test_df.replace('self-portrait', 'portrait')\n",
    "test_df = test_df.replace('poster', 'design')\n",
    "test_df = test_df.replace('advertisement', 'illustration')\n",
    "test_df = test_df.replace('cloudscape', 'landscape')\n",
    "test_df = test_df.replace('literary painting', 'mythological painting')\n",
    "test_df = test_df.replace('battle painting', 'history painting')\n",
    "test_df = test_df.replace('bird-and-flower painting', 'animal painting')\n",
    "test_df = test_df[test_df.genre.isin(['shan shui','panorama','miniature','pastorale','quadratura','vanitas','bijinga',\n",
    "                                        'calligraphy','yakusha-e'])==False]\n",
    "#valid_df = valid_df[valid_df.genre.isin(['history painting','allegorical painting','interior','capriccio','veduta','caricature','tessellation'])==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_genre = {v: k for k, v in class_dict_genre.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_medium=pd.read_pickle('./train_medium.pkl')\n",
    "#valid_df_medium=pd.read_pickle('./valid_medium.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "dict=Counter(train_df_original.genre)\n",
    "sorted_dict = sorted(dict.items(), key=lambda x: x[1])\n",
    "first_dict = sorted_dict[-10:]\n",
    "list_genre=[]\n",
    "dict_iter = iter(first_dict)\n",
    "for (name, count) in dict_iter:\n",
    "    list_genre.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df = train_df_original.loc[train_df_original['genre'].isin(list_genre)] #55545 immagini\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_artist = {}\n",
    "for i, artist in enumerate(np.sort(train_df.artist.unique())):\n",
    "    train_df.loc[train_df.artist==artist, 'class_artist'] = i\n",
    "    test_df.loc[test_df.artist==artist, 'class_artist'] = i\n",
    "    class_dict_artist.update({i:artist})\n",
    "\n",
    "class_dict_style = {}\n",
    "for i, style in enumerate(np.sort(train_df['style'].unique())):\n",
    "    train_df.loc[train_df['style']==style, 'class_style'] = i\n",
    "    test_df.loc[test_df['style']==style, 'class_style'] = i\n",
    "    class_dict_style.update({i:style})\n",
    "    \n",
    "class_dict_genre = {}\n",
    "for i, genre in enumerate(np.sort(train_df.genre.unique())):\n",
    "    train_df.loc[train_df.genre==genre, 'class_genre'] = i\n",
    "    test_df.loc[test_df.genre==genre, 'class_genre'] = i\n",
    "    class_dict_genre.update({i:genre})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_dict_medium = {}\n",
    "#for i, medium in enumerate(np.sort(train_df_medium.medium.unique())):\n",
    "#    train_df_medium.loc[train_df_medium.medium==medium, 'class_medium'] = i\n",
    "#    test_df_medium.loc[test_df_medium.medium==medium, 'class_medium'] = i\n",
    "#    class_dict_medium.update({i:medium})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "        T.Resize(256), \n",
    "        T.RandomResizedCrop(size=224, scale=(0.3,1), ratio=(1, 1)), #size 384. scale specifies the lower and upper bounds for the random area of the crop\n",
    "        T.RandomHorizontalFlip(p=0.5), #p probability of the image being flipped\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                    std=[0.229, 0.224, 0.225])])\n",
    "test_transforms = T.Compose([\n",
    "        T.Resize(224), #T.Resize((331,331)), T.CenterCrop(300) per efficientnet\n",
    "        T.CenterCrop(224), \n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                    std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletCSNDataset(Dataset): #genre, artist, style\n",
    "    \n",
    "    def __init__(self, df, transform, sample=None):\n",
    "        if sample: \n",
    "            self.df = df.groupby(by='genre').sample(sample)\n",
    "        else:\n",
    "            self.df = df\n",
    "            \n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cond_genre = 0\n",
    "        cond_artist = 1\n",
    "        cond_style = 2\n",
    "        anc = self.df.iloc[idx] \n",
    "        ancImg = Image.open(anc.img_path).convert('RGB')\n",
    "        \n",
    "        pos_genre = self.df[self.df.genre == anc.genre].sample(1).squeeze()\n",
    "        neg_genre = self.df[self.df.genre != anc.genre].sample(1).squeeze()\n",
    "        \n",
    "        pos_artist = self.df[self.df.artist == anc.artist].sample(1).squeeze()\n",
    "        neg_artist = self.df[self.df.artist != anc.artist].sample(1).squeeze()\n",
    "        \n",
    "        pos_style = self.df[self.df['style'] == anc['style']].sample(1).squeeze()\n",
    "        neg_style = self.df[self.df['style'] != anc['style']].sample(1).squeeze()\n",
    "        \n",
    "        posImg_genre = Image.open(pos_genre.img_path).convert('RGB')\n",
    "        negImg_genre = Image.open(neg_genre.img_path).convert('RGB')\n",
    "        \n",
    "        posImg_artist = Image.open(pos_artist.img_path).convert('RGB')\n",
    "        negImg_artist = Image.open(neg_artist.img_path).convert('RGB')\n",
    "        \n",
    "        posImg_style = Image.open(pos_style.img_path).convert('RGB')\n",
    "        negImg_style = Image.open(neg_style.img_path).convert('RGB')\n",
    "          \n",
    "        anc_genre = [int(anc.class_genre)]\n",
    "        anc_artist = [int(anc.class_artist)]\n",
    "        anc_style = [int(anc.class_style)]\n",
    "        \n",
    "        #triplet (anchor, far, close)\n",
    "        return self.transform(ancImg), self.transform(negImg_genre), self.transform(posImg_genre), cond_genre, self.transform(negImg_artist), self.transform(posImg_artist), cond_artist, self.transform(negImg_style), self.transform(posImg_style), cond_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class TripletCSNDataset(Dataset): #genre, artist, style\n",
    "    \n",
    "    def __init__(self, df, df2, transform):\n",
    "        self.df  = df\n",
    "        self.df2  = df2\n",
    "        self.transform  = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)+len(self.df2)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cond_genre = 0\n",
    "        cond_artist = 1\n",
    "        cond_style = 2\n",
    "        cond_medium = 3\n",
    "        row = self.df.loc[idx]\n",
    "        row_medium = self.df2.loc[idx]\n",
    "        anchor = Image.open(self.df.img_path.loc[idx]).convert('RGB')\n",
    "        anchor_medium = Image.open(self.df2.img_path.loc[idx]).convert('RGB')\n",
    "        anchor = self.transform(anchor)\n",
    "        anchor_medium = self.transform(anchor_medium)\n",
    "        \n",
    "        g_pos = Image.open(self.df[self.df.class_genre==row.class_genre].drop(row.name).sample(1).iloc[0].img_path).convert('RGB')\n",
    "        g_pos = self.transform(g_positive)\n",
    "        g_neg = Image.open(self.df[self.df.class_genre!=row.class_genre].sample(1).iloc[0].img_path).convert('RGB')\n",
    "        g_neg = self.transform(g_negative)\n",
    "        anchor_genre = [int(row.class_genre)]\n",
    "        \n",
    "        a_pos = Image.open(self.df[self.df.class_artist==row.class_artist].drop(row.name).sample(1).iloc[0].img_path).convert('RGB')\n",
    "        a_pos = self.transform(a_positive)\n",
    "        a_neg = Image.open(self.df[self.df.class_artist!=row.class_artist].sample(1).iloc[0].img_path).convert('RGB')\n",
    "        a_neg = self.transform(a_negative)    \n",
    "        anchor_artist = [int(row.class_artist)]\n",
    "        \n",
    "        s_pos = Image.open(self.df[self.df.class_style==row.class_style].drop(row.name).sample(1).iloc[0].img_path).convert('RGB')\n",
    "        s_pos = self.transform(s_positive)\n",
    "        s_neg = Image.open(self.df[self.df.class_style!=row.class_style].sample(1).iloc[0].img_path).convert('RGB')\n",
    "        s_neg = self.transform(s_negative)    \n",
    "        anchor_style = [int(row.class_style)]\n",
    "        \n",
    "        m_pos = Image.open(self.df2[self.df2.class_medium==row.class_medium].drop(row.name).sample(1).iloc[0].img_path).convert('RGB')\n",
    "        m_pos = self.transform(m_pos)\n",
    "        m_neg = Image.open(self.df2[self.df2.class_medium!=row.class_medium].sample(1).iloc[0].img_path).convert('RGB')\n",
    "        m_neg = self.transform(m_neg)\n",
    "        anchor_medium = [int(row_medium.class_medium)]\n",
    "        #triplet (anchor, far, close)\n",
    "        return anchor, g_neg, g_pos, cond_genre, a_neg, a_pos, cond_artist, s_neg, s_pos, cond_style, row, anchor_medium, m_neg, m_pos, row_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripletCSNDataset(train_df.reset_index(drop=True), train_transforms)    \n",
    "test_dataset = TripletCSNDataset(test_df.reset_index(drop=True), test_transforms, sample=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset = TripletCSNDataset(train_df.reset_index(drop=True), train_df_medium.reset_index(drop=True), train_transforms)    \n",
    "test_dataset = TripletCSNDataset(test_df.reset_index(drop=True), test_df_medium.reset_index(drop=True), test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #16, 256\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=False, num_workers=6)\n",
    "test_loader  = DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=False, num_workers=6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions CSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CS_Tripletnet(nn.Module):\n",
    "    def __init__(self, embeddingnet):\n",
    "        super(CS_Tripletnet, self).__init__()\n",
    "        self.embeddingnet = embeddingnet\n",
    "\n",
    "    def forward(self, x, y, z, c):\n",
    "        \"\"\" x: Anchor image,\n",
    "            y: Distant (negative) image,\n",
    "            z: Close (positive) image,\n",
    "            c: Integer indicating according to which notion of similarity images are compared\"\"\"\n",
    "        embedded_x, masknorm_norm_x, embed_norm_x, tot_embed_norm_x = self.embeddingnet(x, c)\n",
    "        embedded_y, masknorm_norm_y, embed_norm_y, tot_embed_norm_y = self.embeddingnet(y, c)\n",
    "        embedded_z, masknorm_norm_z, embed_norm_z, tot_embed_norm_z = self.embeddingnet(z, c)\n",
    "        mask_norm = (masknorm_norm_x + masknorm_norm_y + masknorm_norm_z) / 3\n",
    "        embed_norm = (embed_norm_x + embed_norm_y + embed_norm_z) / 3\n",
    "        mask_embed_norm = (tot_embed_norm_x + tot_embed_norm_y + tot_embed_norm_z) / 3\n",
    "        dist_a = F.pairwise_distance(embedded_x, embedded_y, 2) #distanza anchor-negative (norma2)\n",
    "        dist_b = F.pairwise_distance(embedded_x, embedded_z, 2) #distanza anchor-positive\n",
    "        return dist_a, dist_b, mask_norm, embed_norm, mask_embed_norm, embedded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSimNet(nn.Module):\n",
    "    def __init__(self, embeddingnet, n_conditions, embedding_size, learnedmask=True, prein=False): #embeddingnet=resnet18\n",
    "        \"\"\" embeddingnet: The network that projects the inputs into an embedding of embedding_size\n",
    "            n_conditions: Integer defining number of different similarity notions\n",
    "            embedding_size: Number of dimensions of the embedding output from the embeddingnet\n",
    "            learnedmask: Boolean indicating whether masks are learned or fixed\n",
    "            prein: Boolean indicating whether masks are initialized in equally sized disjoint \n",
    "                sections or random otherwise\"\"\"\n",
    "        super(ConditionalSimNet, self).__init__()\n",
    "        self.learnedmask = learnedmask\n",
    "        self.embeddingnet = embeddingnet\n",
    "        # create the mask\n",
    "        if learnedmask:\n",
    "            if prein:\n",
    "                # define masks \n",
    "                self.masks = torch.nn.Embedding(n_conditions, embedding_size)\n",
    "                # initialize masks\n",
    "                mask_array = np.zeros([n_conditions, embedding_size])\n",
    "                mask_array.fill(0.1)\n",
    "                mask_len = int(embedding_size / n_conditions)\n",
    "                for i in range(n_conditions):\n",
    "                    mask_array[i, i*mask_len:(i+1)*mask_len] = 1\n",
    "                # no gradients for the masks\n",
    "                self.masks.weight = torch.nn.Parameter(torch.Tensor(mask_array), requires_grad=True)\n",
    "            else:\n",
    "                # define masks with gradients\n",
    "                self.masks = torch.nn.Embedding(n_conditions, embedding_size)\n",
    "                # initialize weights\n",
    "                self.masks.weight.data.normal_(0.9, 0.7) # 0.1, 0.005\n",
    "        else:\n",
    "            # define masks \n",
    "            self.masks = torch.nn.Embedding(n_conditions, embedding_size)\n",
    "            # initialize masks\n",
    "            mask_array = np.zeros([n_conditions, embedding_size])\n",
    "            mask_len = int(embedding_size / n_conditions)\n",
    "            for i in range(n_conditions):\n",
    "                mask_array[i, i*mask_len:(i+1)*mask_len] = 1\n",
    "            # no gradients for the masks\n",
    "            self.masks.weight = torch.nn.Parameter(torch.Tensor(mask_array), requires_grad=False)\n",
    "    def forward(self, x, c):\n",
    "        embedded_x = self.embeddingnet(x)\n",
    "        self.mask = self.masks(c)\n",
    "        if self.learnedmask:\n",
    "            self.mask = torch.nn.functional.relu(self.mask)\n",
    "        masked_embedding = embedded_x * self.mask\n",
    "        return masked_embedding, self.mask.norm(1), embedded_x.norm(2), masked_embedding.norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Triplet_model(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Triplet_model, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Linear(2048, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.6), #try 0.6\n",
    "            nn.Linear(256, 128))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_fig(loss_train, loss_valid, accuracy_train, accuracy_valid, epoch):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,4))\n",
    "    ax1.plot([i for i in range(len(loss_train))], loss_train, label='train_loss')\n",
    "    ax1.plot([i for i in range(len(loss_valid))],  loss_valid,  label='valid_loss')\n",
    "    ax1.set(xlabel='epoch', ylabel='loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot([i for i in range(len(accuracy_train))], accuracy_train, label='train_accuracy')\n",
    "    ax2.plot([i for i in range(len(accuracy_valid))],  accuracy_valid,  label='valid_accuracy')\n",
    "    ax2.set(xlabel='epoch', ylabel='accuracy')\n",
    "    ax2.legend()\n",
    "    fig.suptitle(f'EPOCH {epoch}', fontsize=16)\n",
    "    plt.close(fig)\n",
    "    fig.savefig(os.path.join('results_csn', \"loss_plot_csn_SGD.jpg\"), pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dist_an, dist_ap):\n",
    "    margin = 0\n",
    "    pred = (dist_an - dist_ap - margin).cpu().data\n",
    "    return (pred > 0).sum()*1.0/dist_an.size()[0], (pred > 0).sum()*1.0\n",
    "\n",
    "def accuracy_id(dist_an, dist_ap, c, c_id):\n",
    "    margin = 0\n",
    "    pred = (dist_an - dist_ap - margin).cpu().data\n",
    "    return ((pred > 0)*(c.cpu().data == c_id)).sum()*1.0/(c.cpu().data == c_id).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    npimg = image.numpy().transpose(1, 2, 0)\n",
    "    npimg = npimg/(npimg.max()-npimg.min())+0.5\n",
    "    plt.imshow(npimg)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.resnet50(pretrained=True)\n",
    "#base_model = encoding.models.get_model('ResNeSt50', pretrained=True)\n",
    "base_model.fc = nn.Identity()\n",
    "model = Triplet_model(base_model)\n",
    "#model.float()\n",
    "csn_model = ConditionalSimNet(model, n_conditions=3, embedding_size=128, learnedmask=True, prein=False) #mask_weight=normal(0.9, 0.7)\n",
    "tnet = CS_Tripletnet(csn_model)\n",
    "tnet = tnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MarginRankingLoss(margin = 0.2)\n",
    "#optimizer = torch.optim.Adam([\n",
    "                             # {'params':tnet.embeddingnet.embeddingnet.base_model.parameters(), 'lr':1.e-7},\n",
    "                              #{'params':tnet.embeddingnet.embeddingnet.fc.parameters(),   'lr':1.e-4}\n",
    "                              #], lr=1.e-5)\n",
    "optimizer = torch.optim.SGD([{'params':tnet.embeddingnet.embeddingnet.base_model.parameters(), 'lr':1.e-7},{'params':tnet.embeddingnet.embeddingnet.fc.parameters(),   'lr':1.e-4}], lr=1.e-4, momentum=0.9, weight_decay=0.0001)\n",
    "#adam alfa=5E-5, beta1=0.1, beta2=0.001\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.90)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(train_loader, tnet, criterion, optimizer, epoch, device, embed_loss=5.e-3, mask_loss=5.e-4):\n",
    "    losses_genre, losses_artist, losses_style = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    accs_genre, accs_artist, accs_style = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    correct_genre, correct_artist, correct_style = 0, 0, 0\n",
    "    \n",
    "    tnet.train()\n",
    "    for batch_idx, (anchor, g_negative, g_positive, condition_genre, a_negative, a_positive, condition_artist, s_negative, s_positive, condition_style) in enumerate(train_loader):  \n",
    "        \n",
    "        anchor, g_negative, g_positive, condition_genre = anchor.to(device), g_negative.to(device), g_positive.to(device), condition_genre.to(device)\n",
    "        a_negative, a_positive, condition_artist = a_negative.to(device), a_positive.to(device), condition_artist.to(device)\n",
    "        s_negative, s_positive, condition_style = s_negative.to(device), s_positive.to(device), condition_style.to(device)\n",
    "        \n",
    "        dist_an_genre, dist_ap_genre, mask_norm_genre, embed_norm_genre, mask_embed_norm_genre, anchor_embedded = tnet(anchor, g_negative, g_positive, condition_genre)\n",
    "        dist_an_artist, dist_ap_artist, mask_norm_artist, embed_norm_artist, mask_embed_norm_artist, _ = tnet(anchor, a_negative, a_positive, condition_artist)\n",
    "        dist_an_style, dist_ap_style, mask_norm_style, embed_norm_style, mask_embed_norm_style, _ = tnet(anchor, s_negative, s_positive, condition_style)\n",
    "        \n",
    "        target_genre = torch.FloatTensor(dist_an_genre.size()).fill_(1) # 1 means, dist_an should be larger than dist_ap\n",
    "        target_genre = target_genre.to(device)\n",
    "        target_artist = torch.FloatTensor(dist_an_artist.size()).fill_(1)\n",
    "        target_artist = target_artist.to(device)\n",
    "        target_style = torch.FloatTensor(dist_an_style.size()).fill_(1)\n",
    "        target_style = target_style.to(device)\n",
    "        \n",
    "        loss_triplet_genre = criterion(dist_an_genre, dist_ap_genre, target_genre)\n",
    "        loss_embedd_genre = embed_norm_genre / np.sqrt(anchor.size(0))\n",
    "        loss_mask_genre = mask_norm_genre / anchor.size(0)\n",
    "        loss_genre = loss_triplet_genre + embed_loss * loss_embedd_genre + mask_loss * loss_mask_genre\n",
    "        \n",
    "        loss_triplet_artist = criterion(dist_an_artist, dist_ap_artist, target_artist)\n",
    "        loss_embedd_artist = embed_norm_artist / np.sqrt(anchor.size(0))\n",
    "        loss_mask_artist = mask_norm_artist / anchor.size(0)\n",
    "        loss_artist = loss_triplet_artist + embed_loss * loss_embedd_artist + mask_loss * loss_mask_artist\n",
    "        \n",
    "        loss_triplet_style = criterion(dist_an_style, dist_ap_style, target_style)\n",
    "        loss_embedd_style = embed_norm_style / np.sqrt(anchor.size(0))\n",
    "        loss_mask_style = mask_norm_style / anchor.size(0)\n",
    "        loss_style = loss_triplet_style + embed_loss * loss_embedd_style + mask_loss * loss_mask_style\n",
    "                \n",
    "        loss_tot = (loss_genre+loss_artist+loss_style)/3\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        acc_genre, label_acc_genre = accuracy(dist_an_genre, dist_ap_genre)\n",
    "        acc_artist, label_acc_artist = accuracy(dist_an_artist, dist_ap_artist)\n",
    "        acc_style, label_acc_style = accuracy(dist_an_style, dist_ap_style)\n",
    "        \n",
    "        correct_genre = correct_genre + label_acc_genre.item()\n",
    "        correct_artist = correct_artist + label_acc_artist.item()\n",
    "        correct_style = correct_style + label_acc_style.item()\n",
    "        \n",
    "        losses_genre.update(loss_triplet_genre.data.item(), anchor.size(0))\n",
    "        losses_artist.update(loss_triplet_artist.data.item(), anchor.size(0))\n",
    "        losses_style.update(loss_triplet_style.data.item(), anchor.size(0))\n",
    "        \n",
    "        accs_genre.update(acc_genre, anchor.size(0))\n",
    "        accs_artist.update(acc_artist, anchor.size(0))\n",
    "        accs_style.update(acc_style, anchor.size(0))\n",
    "        \n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss_tot.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "        template = f'Iteration [{batch_idx}/{len(train_loader)}] Training | Train Loss: {round((losses_genre.avg + losses_artist.avg + losses_style.avg)/3,3)} | Accuracy: {round(((accs_genre.avg + accs_artist.avg + accs_style.avg)/3).item(),3)} | Correct Genre: {round(accs_genre.avg.item(),3)} | Correct Artist: {round(accs_artist.avg.item(),3)} | Correct Style: {round(accs_style.avg.item(),3)}\\r'\n",
    "        print(template, end='')\n",
    "        \n",
    "    mean_accs = (accs_genre.avg + accs_artist.avg + accs_style.avg)/3\n",
    "    mean_losses = (losses_genre.avg + losses_artist.avg + losses_style.avg)/3\n",
    "    \n",
    "    return tnet, mean_accs, mean_losses, int(correct_genre), int(correct_artist), int(correct_style), accs_genre.avg, accs_artist.avg, accs_style.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_func(valid_loader, tnet, criterion, epoch, device, embed_loss=1.e-4, mask_loss=1e-4):\n",
    "    \n",
    "    losses_genre, losses_artist, losses_style = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    accs_genre, accs_artist, accs_style = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    correct_genre, correct_artist, correct_style = 0, 0, 0\n",
    "    anchors_embedded = []\n",
    "    \n",
    "    tnet.eval()  \n",
    "    for batch_idx, (anchor, g_negative, g_positive, condition_genre, a_negative, a_positive, condition_artist, s_negative, s_positive, condition_style) in enumerate(valid_loader):\n",
    "        \n",
    "        anchor, g_negative, g_positive, condition_genre = anchor.to(device), g_negative.to(device), g_positive.to(device), condition_genre.to(device)\n",
    "        a_negative, a_positive, condition_artist = a_negative.to(device), a_positive.to(device), condition_artist.to(device)\n",
    "        s_negative, s_positive, condition_style = s_negative.to(device), s_positive.to(device), condition_style.to(device)\n",
    "        \n",
    "        dist_an_genre, dist_ap_genre, _, _, _, anchor_embedded = tnet(anchor, g_negative, g_positive, condition_genre)\n",
    "        dist_an_artist, dist_ap_artist, _, _, _, _ = tnet(anchor, a_negative, a_positive, condition_artist)\n",
    "        dist_an_style, dist_ap_style, _, _, _, _ = tnet(anchor, s_negative, s_positive, condition_style)\n",
    "        \n",
    "        anchors_embedded.append(anchor_embedded.detach().cpu().numpy())\n",
    "        #anchors_row.append(anchor_row.detach().cpu().numpy())\n",
    "        \n",
    "        target_genre = torch.FloatTensor(dist_an_genre.size()).fill_(1)\n",
    "        target_genre = target_genre.to(device)\n",
    "        target_artist = torch.FloatTensor(dist_an_artist.size()).fill_(1)\n",
    "        target_artist = target_artist.to(device)\n",
    "        target_style = torch.FloatTensor(dist_an_style.size()).fill_(1)\n",
    "        target_style = target_style.to(device)\n",
    "        \n",
    "        valid_loss_genre = criterion(dist_an_genre, dist_ap_genre, target_genre)\n",
    "        valid_loss_artist = criterion(dist_an_artist, dist_ap_artist, target_artist)\n",
    "        valid_loss_style = criterion(dist_an_style, dist_ap_style, target_style)\n",
    "        \n",
    "        acc_genre, label_acc_genre = accuracy(dist_an_genre, dist_ap_genre)\n",
    "        acc_artist, label_acc_artist = accuracy(dist_an_artist, dist_ap_artist)\n",
    "        acc_style, label_acc_style = accuracy(dist_an_style, dist_ap_style)\n",
    "        \n",
    "        correct_genre = correct_genre + label_acc_genre.item()\n",
    "        correct_artist = correct_artist + label_acc_artist.item()\n",
    "        correct_style = correct_style + label_acc_style.item()\n",
    "        \n",
    "        accs_genre.update(acc_genre, anchor.size(0))\n",
    "        accs_artist.update(acc_artist, anchor.size(0))\n",
    "        accs_style.update(acc_style, anchor.size(0))\n",
    "        \n",
    "        losses_genre.update(valid_loss_genre.data.item(), anchor.size(0))\n",
    "        losses_artist.update(valid_loss_artist.data.item(), anchor.size(0))\n",
    "        losses_style.update(valid_loss_style.data.item(), anchor.size(0))\n",
    "        \n",
    "        template = f'Iteration [{batch_idx}/{len(valid_loader)}] Validating | Valid Loss: {round(((losses_genre.avg + losses_artist.avg + losses_style.avg)/3),3)} | Accuracy: {round(((accs_genre.avg + accs_artist.avg + accs_style.avg)/3).item(),3)} | Correct Genre: {round(accs_genre.avg.item(),3)} | Correct Artist: {round(accs_artist.avg.item(),3)} | Correct Style: {round(accs_style.avg.item(),3)}\\r'\n",
    "        print(template, end='')\n",
    "\n",
    "    mean_accs = (accs_genre.avg + accs_artist.avg + accs_style.avg)/3\n",
    "    mean_losses = (losses_genre.avg + losses_artist.avg + losses_style.avg)/3\n",
    "        \n",
    "    return mean_accs, mean_losses, int(correct_genre), int(correct_artist), int(correct_style), accs_genre.avg, accs_artist.avg, accs_style.avg, anchors_embedded#, anchors_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_los = 10\n",
    "loss_train, loss_valid = [], []\n",
    "accuracy_train, accuracy_valid = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t| Train Loss: 0.21 | Accuracy: 0.541 | Correct Genre: 0.535 | Correct Artist: 0.572 | Correct Style: 0.517 | Correct Style: 0.604\n",
      "        \t\t| Valid Loss: 0.142 | Accuracy: 0.681 | Correct Genre: 0.592 | Correct Artist: 0.847 | Correct Style: 0.604\n",
      "\n",
      "------------------- Saving model -------------------\n",
      "Epoch: 1\t| Train Loss: 0.188 | Accuracy: 0.565 | Correct Genre: 0.539 | Correct Artist: 0.619 | Correct Style: 0.537| Correct Style: 0.612\n",
      "        \t\t| Valid Loss: 0.141 | Accuracy: 0.687 | Correct Genre: 0.604 | Correct Artist: 0.846 | Correct Style: 0.612\n",
      "\n",
      "------------------- Saving model -------------------\n",
      "Epoch: 2\t| Train Loss: 0.186 | Accuracy: 0.578 | Correct Genre: 0.545 | Correct Artist: 0.646 | Correct Style: 0.544rrect Style: 0.6477646\n",
      "        \t\t| Valid Loss: 0.14 | Accuracy: 0.7 | Correct Genre: 0.634 | Correct Artist: 0.82 | Correct Style: 0.647\n",
      "\n",
      "------------------- Saving model -------------------\n",
      "Epoch: 3\t| Train Loss: 0.178 | Accuracy: 0.593 | Correct Genre: 0.565 | Correct Artist: 0.667 | Correct Style: 0.548| Correct Style: 0.669\n",
      "        \t\t| Valid Loss: 0.134 | Accuracy: 0.711 | Correct Genre: 0.629 | Correct Artist: 0.836 | Correct Style: 0.669\n",
      "\n",
      "------------------- Saving model -------------------\n",
      "Epoch: 4\t| Train Loss: 0.172 | Accuracy: 0.61 | Correct Genre: 0.58 | Correct Artist: 0.682 | Correct Style: 0.5697 | Correct Style: 0.651\n",
      "        \t\t| Valid Loss: 0.143 | Accuracy: 0.713 | Correct Genre: 0.642 | Correct Artist: 0.847 | Correct Style: 0.65\n",
      "\n",
      "Epoch: 5\t| Train Loss: 0.173 | Accuracy: 0.611 | Correct Genre: 0.574 | Correct Artist: 0.685 | Correct Style: 0.575| Correct Style: 0.616\n",
      "        \t\t| Valid Loss: 0.145 | Accuracy: 0.703 | Correct Genre: 0.636 | Correct Artist: 0.858 | Correct Style: 0.616\n",
      "\n",
      "Epoch: 6\t| Train Loss: 0.165 | Accuracy: 0.633 | Correct Genre: 0.623 | Correct Artist: 0.7 | Correct Style: 0.5753 | Correct Style: 0.678\n",
      "        \t\t| Valid Loss: 0.137 | Accuracy: 0.727 | Correct Genre: 0.659 | Correct Artist: 0.843 | Correct Style: 0.678\n",
      "\n",
      "Epoch: 7\t| Train Loss: 0.166 | Accuracy: 0.633 | Correct Genre: 0.584 | Correct Artist: 0.715 | Correct Style: 0.599| Correct Style: 0.632\n",
      "        \t\t| Valid Loss: 0.151 | Accuracy: 0.716 | Correct Genre: 0.675 | Correct Artist: 0.842 | Correct Style: 0.632\n",
      "\n",
      "Epoch: 8\t| Train Loss: 0.16 | Accuracy: 0.64 | Correct Genre: 0.605 | Correct Artist: 0.718 | Correct Style: 0.5984 | Correct Style: 0.686\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.734 | Correct Genre: 0.671 | Correct Artist: 0.844 | Correct Style: 0.686\n",
      "\n",
      "Epoch: 9\t| Train Loss: 0.159 | Accuracy: 0.648 | Correct Genre: 0.594 | Correct Artist: 0.745 | Correct Style: 0.606 Correct Style: 0.6519\n",
      "        \t\t| Valid Loss: 0.14 | Accuracy: 0.737 | Correct Genre: 0.685 | Correct Artist: 0.875 | Correct Style: 0.651\n",
      "\n",
      "Epoch: 10\t| Train Loss: 0.159 | Accuracy: 0.65 | Correct Genre: 0.62 | Correct Artist: 0.731 | Correct Style: 0.598| Correct Style: 0.6887\n",
      "        \t\t| Valid Loss: 0.142 | Accuracy: 0.74 | Correct Genre: 0.677 | Correct Artist: 0.855 | Correct Style: 0.688\n",
      "\n",
      "Epoch: 11\t| Train Loss: 0.161 | Accuracy: 0.651 | Correct Genre: 0.623 | Correct Artist: 0.735 | Correct Style: 0.594 Correct Style: 0.691\n",
      "        \t\t| Valid Loss: 0.136 | Accuracy: 0.746 | Correct Genre: 0.682 | Correct Artist: 0.864 | Correct Style: 0.691\n",
      "\n",
      "Epoch: 12\t| Train Loss: 0.158 | Accuracy: 0.652 | Correct Genre: 0.625 | Correct Artist: 0.738 | Correct Style: 0.593Correct Style: 0.7011\n",
      "        \t\t| Valid Loss: 0.14 | Accuracy: 0.745 | Correct Genre: 0.667 | Correct Artist: 0.868 | Correct Style: 0.701\n",
      "\n",
      "Epoch: 13\t| Train Loss: 0.156 | Accuracy: 0.655 | Correct Genre: 0.619 | Correct Artist: 0.748 | Correct Style: 0.597Correct Style: 0.6599\n",
      "        \t\t| Valid Loss: 0.15 | Accuracy: 0.735 | Correct Genre: 0.684 | Correct Artist: 0.862 | Correct Style: 0.659\n",
      "\n",
      "Epoch: 14\t| Train Loss: 0.156 | Accuracy: 0.661 | Correct Genre: 0.637 | Correct Artist: 0.738 | Correct Style: 0.609 Correct Style: 0.678\n",
      "        \t\t| Valid Loss: 0.143 | Accuracy: 0.741 | Correct Genre: 0.678 | Correct Artist: 0.868 | Correct Style: 0.678\n",
      "\n",
      "Epoch: 15\t| Train Loss: 0.154 | Accuracy: 0.658 | Correct Genre: 0.605 | Correct Artist: 0.758 | Correct Style: 0.612 Correct Style: 0.658\n",
      "        \t\t| Valid Loss: 0.154 | Accuracy: 0.723 | Correct Genre: 0.665 | Correct Artist: 0.847 | Correct Style: 0.658\n",
      "\n",
      "Epoch: 16\t| Train Loss: 0.151 | Accuracy: 0.673 | Correct Genre: 0.63 | Correct Artist: 0.752 | Correct Style: 0.635| Correct Style: 0.682\n",
      "        \t\t| Valid Loss: 0.141 | Accuracy: 0.746 | Correct Genre: 0.697 | Correct Artist: 0.859 | Correct Style: 0.682\n",
      "\n",
      "Epoch: 17\t| Train Loss: 0.149 | Accuracy: 0.672 | Correct Genre: 0.628 | Correct Artist: 0.771 | Correct Style: 0.618Correct Style: 0.6899\n",
      "        \t\t| Valid Loss: 0.151 | Accuracy: 0.75 | Correct Genre: 0.691 | Correct Artist: 0.869 | Correct Style: 0.689\n",
      "\n",
      "Epoch: 18\t| Train Loss: 0.146 | Accuracy: 0.683 | Correct Genre: 0.636 | Correct Artist: 0.78 | Correct Style: 0.634| Correct Style: 0.652\n",
      "        \t\t| Valid Loss: 0.147 | Accuracy: 0.734 | Correct Genre: 0.686 | Correct Artist: 0.866 | Correct Style: 0.652\n",
      "\n",
      "Epoch: 19\t| Train Loss: 0.146 | Accuracy: 0.683 | Correct Genre: 0.654 | Correct Artist: 0.778 | Correct Style: 0.618 Correct Style: 0.699\n",
      "        \t\t| Valid Loss: 0.149 | Accuracy: 0.738 | Correct Genre: 0.658 | Correct Artist: 0.858 | Correct Style: 0.699\n",
      "\n",
      "Epoch: 20\t| Train Loss: 0.154 | Accuracy: 0.661 | Correct Genre: 0.615 | Correct Artist: 0.769 | Correct Style: 0.6 | Correct Style: 0.672\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.744 | Correct Genre: 0.684 | Correct Artist: 0.875 | Correct Style: 0.672\n",
      "\n",
      "Epoch: 21\t| Train Loss: 0.146 | Accuracy: 0.685 | Correct Genre: 0.65 | Correct Artist: 0.78 | Correct Style: 0.625| Correct Style: 0.6856\n",
      "        \t\t| Valid Loss: 0.142 | Accuracy: 0.746 | Correct Genre: 0.68 | Correct Artist: 0.872 | Correct Style: 0.685\n",
      "\n",
      "Epoch: 22\t| Train Loss: 0.152 | Accuracy: 0.668 | Correct Genre: 0.622 | Correct Artist: 0.759 | Correct Style: 0.623 Correct Style: 0.661\n",
      "        \t\t| Valid Loss: 0.142 | Accuracy: 0.742 | Correct Genre: 0.698 | Correct Artist: 0.868 | Correct Style: 0.66\n",
      "\n",
      "Epoch: 23\t| Train Loss: 0.154 | Accuracy: 0.663 | Correct Genre: 0.631 | Correct Artist: 0.761 | Correct Style: 0.599 Correct Style: 0.687\n",
      "        \t\t| Valid Loss: 0.141 | Accuracy: 0.739 | Correct Genre: 0.665 | Correct Artist: 0.866 | Correct Style: 0.687\n",
      "\n",
      "Epoch: 24\t| Train Loss: 0.15 | Accuracy: 0.678 | Correct Genre: 0.636 | Correct Artist: 0.786 | Correct Style: 0.613| Correct Style: 0.692\n",
      "        \t\t| Valid Loss: 0.138 | Accuracy: 0.763 | Correct Genre: 0.712 | Correct Artist: 0.887 | Correct Style: 0.692\n",
      "\n",
      "Epoch: 25\t| Train Loss: 0.15 | Accuracy: 0.669 | Correct Genre: 0.634 | Correct Artist: 0.773 | Correct Style: 0.601| Correct Style: 0.673\n",
      "        \t\t| Valid Loss: 0.149 | Accuracy: 0.743 | Correct Genre: 0.689 | Correct Artist: 0.868 | Correct Style: 0.673\n",
      "\n",
      "Epoch: 26\t| Train Loss: 0.149 | Accuracy: 0.672 | Correct Genre: 0.64 | Correct Artist: 0.752 | Correct Style: 0.625| Correct Style: 0.666\n",
      "        \t\t| Valid Loss: 0.151 | Accuracy: 0.741 | Correct Genre: 0.678 | Correct Artist: 0.879 | Correct Style: 0.666\n",
      "\n",
      "Epoch: 27\t| Train Loss: 0.148 | Accuracy: 0.676 | Correct Genre: 0.628 | Correct Artist: 0.77 | Correct Style: 0.63 | Correct Style: 0.673\n",
      "        \t\t| Valid Loss: 0.154 | Accuracy: 0.747 | Correct Genre: 0.705 | Correct Artist: 0.863 | Correct Style: 0.673\n",
      "\n",
      "Epoch: 28\t| Train Loss: 0.146 | Accuracy: 0.682 | Correct Genre: 0.654 | Correct Artist: 0.785 | Correct Style: 0.608orrect Style: 0.68321\n",
      "        \t\t| Valid Loss: 0.151 | Accuracy: 0.744 | Correct Genre: 0.68 | Correct Artist: 0.87 | Correct Style: 0.683\n",
      "\n",
      "Epoch: 29\t| Train Loss: 0.147 | Accuracy: 0.678 | Correct Genre: 0.624 | Correct Artist: 0.798 | Correct Style: 0.61| Correct Style: 0.689\n",
      "        \t\t| Valid Loss: 0.147 | Accuracy: 0.756 | Correct Genre: 0.708 | Correct Artist: 0.871 | Correct Style: 0.689\n",
      "\n",
      "Epoch: 30\t| Train Loss: 0.146 | Accuracy: 0.684 | Correct Genre: 0.64 | Correct Artist: 0.794 | Correct Style: 0.618| Correct Style: 0.693\n",
      "        \t\t| Valid Loss: 0.143 | Accuracy: 0.754 | Correct Genre: 0.679 | Correct Artist: 0.888 | Correct Style: 0.693\n",
      "\n",
      "Epoch: 31\t| Train Loss: 0.142 | Accuracy: 0.693 | Correct Genre: 0.654 | Correct Artist: 0.801 | Correct Style: 0.625Correct Style: 0.7011\n",
      "        \t\t| Valid Loss: 0.145 | Accuracy: 0.76 | Correct Genre: 0.698 | Correct Artist: 0.881 | Correct Style: 0.701\n",
      "\n",
      "Epoch: 32\t| Train Loss: 0.15 | Accuracy: 0.673 | Correct Genre: 0.632 | Correct Artist: 0.788 | Correct Style: 0.69 | Correct Style: 0.688\n",
      "        \t\t| Valid Loss: 0.153 | Accuracy: 0.748 | Correct Genre: 0.696 | Correct Artist: 0.859 | Correct Style: 0.688\n",
      "\n",
      "Epoch: 33\t| Train Loss: 0.145 | Accuracy: 0.689 | Correct Genre: 0.643 | Correct Artist: 0.771 | Correct Style: 0.653orrect Style: 0.71717\n",
      "        \t\t| Valid Loss: 0.134 | Accuracy: 0.768 | Correct Genre: 0.7 | Correct Artist: 0.887 | Correct Style: 0.717\n",
      "\n",
      "Epoch: 34\t| Train Loss: 0.146 | Accuracy: 0.687 | Correct Genre: 0.643 | Correct Artist: 0.785 | Correct Style: 0.632 Correct Style: 0.688\n",
      "        \t\t| Valid Loss: 0.139 | Accuracy: 0.758 | Correct Genre: 0.704 | Correct Artist: 0.882 | Correct Style: 0.688\n",
      "\n",
      "Epoch: 35\t| Train Loss: 0.143 | Accuracy: 0.689 | Correct Genre: 0.652 | Correct Artist: 0.785 | Correct Style: 0.63 Correct Style: 0.6762\n",
      "        \t\t| Valid Loss: 0.149 | Accuracy: 0.75 | Correct Genre: 0.702 | Correct Artist: 0.872 | Correct Style: 0.676\n",
      "\n",
      "Epoch: 36\t| Train Loss: 0.143 | Accuracy: 0.697 | Correct Genre: 0.664 | Correct Artist: 0.783 | Correct Style: 0.645Correct Style: 0.6899\n",
      "        \t\t| Valid Loss: 0.14 | Accuracy: 0.755 | Correct Genre: 0.692 | Correct Artist: 0.883 | Correct Style: 0.689\n",
      "\n",
      "Epoch: 37\t| Train Loss: 0.145 | Accuracy: 0.692 | Correct Genre: 0.654 | Correct Artist: 0.793 | Correct Style: 0.628 Correct Style: 0.683\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.751 | Correct Genre: 0.697 | Correct Artist: 0.872 | Correct Style: 0.683\n",
      "\n",
      "Epoch: 38\t| Train Loss: 0.145 | Accuracy: 0.687 | Correct Genre: 0.65 | Correct Artist: 0.79 | Correct Style: 0.62 | Correct Style: 0.6699\n",
      "        \t\t| Valid Loss: 0.157 | Accuracy: 0.743 | Correct Genre: 0.691 | Correct Artist: 0.87 | Correct Style: 0.669\n",
      "\n",
      "Epoch: 39\t| Train Loss: 0.142 | Accuracy: 0.689 | Correct Genre: 0.657 | Correct Artist: 0.786 | Correct Style: 0.625 Correct Style: 0.687\n",
      "        \t\t| Valid Loss: 0.149 | Accuracy: 0.752 | Correct Genre: 0.704 | Correct Artist: 0.867 | Correct Style: 0.687\n",
      "\n",
      "Epoch: 40\t| Train Loss: 0.144 | Accuracy: 0.682 | Correct Genre: 0.634 | Correct Artist: 0.793 | Correct Style: 0.618 Correct Style: 0.659\n",
      "        \t\t| Valid Loss: 0.149 | Accuracy: 0.743 | Correct Genre: 0.697 | Correct Artist: 0.873 | Correct Style: 0.659\n",
      "\n",
      "Epoch: 41\t| Train Loss: 0.141 | Accuracy: 0.692 | Correct Genre: 0.671 | Correct Artist: 0.778 | Correct Style: 0.626Correct Style: 0.6921\n",
      "        \t\t| Valid Loss: 0.15 | Accuracy: 0.759 | Correct Genre: 0.715 | Correct Artist: 0.872 | Correct Style: 0.692\n",
      "\n",
      "Epoch: 42\t| Train Loss: 0.139 | Accuracy: 0.699 | Correct Genre: 0.659 | Correct Artist: 0.803 | Correct Style: 0.636 Correct Style: 0.692\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.759 | Correct Genre: 0.704 | Correct Artist: 0.882 | Correct Style: 0.692\n",
      "\n",
      "Epoch: 43\t| Train Loss: 0.141 | Accuracy: 0.696 | Correct Genre: 0.659 | Correct Artist: 0.802 | Correct Style: 0.627 Correct Style: 0.701\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.764 | Correct Genre: 0.714 | Correct Artist: 0.877 | Correct Style: 0.701\n",
      "\n",
      "Epoch: 44\t| Train Loss: 0.143 | Accuracy: 0.691 | Correct Genre: 0.656 | Correct Artist: 0.775 | Correct Style: 0.641 Correct Style: 0.701\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.756 | Correct Genre: 0.692 | Correct Artist: 0.876 | Correct Style: 0.701\n",
      "\n",
      "Epoch: 45\t| Train Loss: 0.145 | Accuracy: 0.683 | Correct Genre: 0.637 | Correct Artist: 0.789 | Correct Style: 0.622 Correct Style: 0.706\n",
      "        \t\t| Valid Loss: 0.144 | Accuracy: 0.759 | Correct Genre: 0.718 | Correct Artist: 0.854 | Correct Style: 0.706\n",
      "\n",
      "Epoch: 46\t| Train Loss: 0.142 | Accuracy: 0.695 | Correct Genre: 0.664 | Correct Artist: 0.794 | Correct Style: 0.628 Correct Style: 0.684\n",
      "        \t\t| Valid Loss: 0.146 | Accuracy: 0.753 | Correct Genre: 0.707 | Correct Artist: 0.867 | Correct Style: 0.684\n",
      "\n",
      "Epoch: 47\t| Train Loss: 0.147 | Accuracy: 0.679 | Correct Genre: 0.642 | Correct Artist: 0.778 | Correct Style: 0.618Correct Style: 0.6921\n",
      "        \t\t| Valid Loss: 0.139 | Accuracy: 0.763 | Correct Genre: 0.71 | Correct Artist: 0.887 | Correct Style: 0.692\n",
      "\n",
      "Epoch: 48\t| Train Loss: 0.143 | Accuracy: 0.69 | Correct Genre: 0.668 | Correct Artist: 0.783 | Correct Style: 0.619| Correct Style: 0.695\n",
      "        \t\t| Valid Loss: 0.143 | Accuracy: 0.754 | Correct Genre: 0.702 | Correct Artist: 0.865 | Correct Style: 0.695\n",
      "\n",
      "Epoch: 49\t| Train Loss: 0.139 | Accuracy: 0.697 | Correct Genre: 0.652 | Correct Artist: 0.788 | Correct Style: 0.651 Correct Style: 0.696\n",
      "        \t\t| Valid Loss: 0.155 | Accuracy: 0.746 | Correct Genre: 0.681 | Correct Artist: 0.863 | Correct Style: 0.696\n",
      "\n",
      "Iteration [522/1200] Training | Train Loss: 0.136 | Accuracy: 0.703 | Correct Genre: 0.668 | Correct Artist: 0.805 | Correct Style: 0.635\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 2000):\n",
    "    \n",
    "    if epoch == 0: acc_valid, losses_valid, correct_genre_valid, correct_genre_valid, correct_style_valid, mean_accs_genre_valid, mean_accs_artist_valid, mean_accs_style_valid, anchors_embedded = valid_func(test_loader, tnet, criterion, epoch, device) \n",
    "\n",
    "    train_loader = DataLoader(TripletCSNDataset(train_df, train_transforms, sample=100),  \n",
    "                          batch_size=batch_size, shuffle=True, \n",
    "                          num_workers = 8, pin_memory=False, \n",
    "                          drop_last=False)\n",
    "    \n",
    "    tnet, acc_train, losses_train, correct_genre_train, correct_artist_train, correct_style_train, mean_accs_genre_train, mean_accs_artist_train, mean_accs_style_train = train_func(train_loader, tnet, criterion, optimizer, epoch, device)\n",
    "    acc_valid, losses_valid, correct_genre_valid, correct_genre_valid, correct_style_valid, mean_accs_genre_valid, mean_accs_artist_valid, mean_accs_style_valid, anchors_embedded = valid_func(test_loader, tnet, criterion, epoch, device)\n",
    "\n",
    "    template = f'Epoch: {epoch}\\t| Train Loss: {round(losses_train,3)} | Accuracy: {round(acc_train.item(),3)} | Correct Genre: {round(mean_accs_genre_train.item(),3)} | Correct Artist: {round(mean_accs_artist_train.item(),3)} | Correct Style: {round(mean_accs_style_train.item(),3)}\\n\\\n",
    "        \\t\\t| Valid Loss: {round(losses_valid,3)} | Accuracy: {round(acc_valid.item(),3)} | Correct Genre: {round(mean_accs_genre_valid.item(),3)} | Correct Artist: {round(mean_accs_artist_valid.item(),3)} | Correct Style: {round(mean_accs_style_valid.item(),3)}\\n'\n",
    "    print(template)\n",
    "    with open(\"results_csn/sample_SGD.txt\", \"a\") as file_object:\n",
    "            file_object.write(template)\n",
    "\n",
    "\n",
    "    if losses_valid < min_los:\n",
    "        min_los = losses_valid\n",
    "        torch.save(tnet, 'csn_model_SGD.pt')\n",
    "        with open(\"results_csn/sample_SGD.txt\", \"a\") as file_object:\n",
    "            file_object.write('------------------- Saving model -------------------\\n')\n",
    "        print('------------------- Saving model -------------------')\n",
    "\n",
    "    loss_train.append(losses_train)\n",
    "    loss_valid.append(losses_valid)\n",
    "    accuracy_train.append(acc_train)\n",
    "    accuracy_valid.append(acc_valid)\n",
    "    save_loss_fig(loss_train, loss_valid, accuracy_train, accuracy_valid, epoch)\n",
    "\n",
    "    scheduler.step(losses_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('csn_model_gsa.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "for i in range(10):\n",
    "    anchor, g_negative, g_positive, condition_genre, a_negative, a_positive, condition_artist, s_negative, s_positive, condition_style=next(iter(test_loader))\n",
    "    anchor, g_negative, g_positive, condition_genre = anchor.to(device), g_negative.to(device), g_positive.to(device), condition_genre.to(device)\n",
    "    dist_an_genre, dist_ap_genre, _, _, _, _ = model(anchor, g_negative, g_positive, condition_genre)\n",
    "    concatenated_an = torch.cat((anchor, g_negative), 0)\n",
    "    concatenated_ap = torch.cat((anchor, g_positive), 0)\n",
    "    #print('Genre anchor: '.format(anchor_row.genre))\n",
    "    imshow(torchvision.utils.make_grid(concatenated_an))\n",
    "    print('Dissimilarity a-n: ', dist_an_genre)\n",
    "    imshow(torchvision.utils.make_grid(concatenated_ap))\n",
    "    print('Dissimilarity a-p: ', dist_ap_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "anchors_embedded, anchors_genre, anchors_path = [],[],[]\n",
    "for anchor, g_negative, g_positive, condition_genre, a_negative, a_positive, condition_artist, s_negative, s_positive, condition_style in next(iter(test_loader)):\n",
    "    anchor, g_negative, g_positive, condition_genre = anchor.to(device), g_negative.to(device), g_positive.to(device), condition_genre.to(device)\n",
    "    dist_an_genre, dist_ap_genre, _, _, _, anchor_embedded = model(anchor, g_negative, g_positive, condition_genre)\n",
    "    anchors_embedded.append(anchor_embedded.detach().cpu().numpy())\n",
    "    anchors_genre.append(row.genre.detach().cpu().numpy())\n",
    "    anchors_path.append(row.img_path.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_image():\n",
    "    tsne = TSNE(n_components=2, perplexity=50, random_state=1) #try different perplexity (between 5 and 50)\n",
    "    X_embedded = tsne.fit_transform(anchors_embedded) #x=anchor_embedded concatenato\n",
    "    tx, ty = tsne[:,0], tsne[:,1]\n",
    "    tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "    ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))\n",
    "\n",
    "    width = 4000\n",
    "    height = 3000\n",
    "    max_dim = 100\n",
    "\n",
    "    full_image = Image.new('RGB', (width, height))\n",
    "    for img, x, y in zip(anchors_path, tx, ty): #change images\n",
    "        tile = Image.open(img)\n",
    "        rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "        tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS) #antialias?\n",
    "        full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGB'))\n",
    "\n",
    "    matplotlib.pyplot.figure(figsize = (16,12))\n",
    "    imshow(full_image)\n",
    "    #https://github.com/sinanatra/image-tsne/blob/master/notebooks/image_tsne.ipynb\n",
    "\n",
    "    full_image.savefig(os.path.join('.', \"tsne_plot_csn.jpg\"), pad_inches=0)\n",
    "\n",
    "        #fig=plt.figure(figsize=(6, 5))\n",
    "        #plt.title('t-SNE')\n",
    "        #target_ids = range(len(list_genre))\n",
    "        #palette = np.array(sns.color_palette(\"hls\", 41))\n",
    "        #colors=palette[colors.astype(np.int)]\n",
    "\n",
    "        #for i, c, label in zip(target_ids, colors, list_genre):\n",
    "         #   plt.scatter(X_embedded[y_train == i, 0], X_embedded[y_train == i, 1], c=c, label=label)\n",
    "        #plt.legend()\n",
    "        #plt.close(fig)\n",
    "        #fig.savefig(os.path.join('.', \"tsne_plot_csn.jpg\"), pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsne_fig(X_train, y_train):\n",
    "    tsne = TSNE(n_components=2, perplexity=50) #try different perplexity (between 5 and 50)\n",
    "    X_embedded = tsne.fit_transform(X_train)\n",
    "    \n",
    "    fig=plt.figure(figsize=(17, 10))\n",
    "    plt.title('t-SNE')\n",
    "        \n",
    "    for label in np.unique(y_train):\n",
    "        plt.scatter(X_embedded[y_train == label, 0], \n",
    "                    X_embedded[y_train == label, 1],\n",
    "                    c=plt.cm.Set1(label / float(len(np.unique(y_train)))),\n",
    "                    alpha=0.8,\n",
    "                    label=class_dict_genre[label])\n",
    "    plt.legend(loc='best')\n",
    "    plt.close(fig)\n",
    "    fig.savefig(os.path.join('.', \"csn_plot.jpg\"), pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truelabels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "\n",
    "for images, target in test_loader:\n",
    "    for label in target.data.numpy():\n",
    "        truelabels.append(label)\n",
    "    out = model(images.to(device))\n",
    "    for prediction in out.data.argmax(1):\n",
    "        predictions.append(prediction.detach().cpu().numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_image_as_np_array(filename: str):\n",
    " #   \"\"\"Returns an image as an numpy array\n",
    "  #  \"\"\"\n",
    "   # img = Image.open(filename)\n",
    "    #return np.asarray(img)\n",
    "\n",
    "\n",
    "def plot_knn_examples(embeddings, dataframe, n_neighbors=4, num_examples=6):\n",
    "    \"\"\"Plots multiple rows of random images with their nearest neighbors\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    # get 5 random samples\n",
    "    samples_idx = np.random.choice(len(indices), size=num_examples, replace=False)\n",
    "\n",
    "    # loop through our randomly picked samples\n",
    "    for idx in samples_idx:\n",
    "        fig = plt.figure()\n",
    "        # loop through their nearest neighbors\n",
    "        for plot_x_offset, neighbor_idx in enumerate(indices[idx]):\n",
    "            # add the subplot\n",
    "            ax = fig.add_subplot(1, len(indices[idx]), plot_x_offset + 1)\n",
    "            # get the correponding filename for the current index\n",
    "            #fname = dataframe.img_path[neighbor_idx]\n",
    "            # plot the image\n",
    "            imshow(Image.open(dataframe.img_path[neighbor_idx]).convert('RGB'))\n",
    "            # set the title to the distance of the neighbor\n",
    "            ax.set_title(f'd={distances[idx][plot_x_offset]:.3f}')\n",
    "            # let's disable the axis\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_examples(anchors_embedded, test_df) #embeddings n samples x n features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "row = test_df.sample(1).iloc[0]\n",
    "print(f'Actual genre: {row.genre}')\n",
    "im = Image.open(row.img_path).convert('RGB')\n",
    "X = []\n",
    "distout = model(valid_transforms(im).unsqueeze(0).to(device))\n",
    "X.append(out.detach().cpu().numpy())\n",
    "pred = clf.predict(np.concatenate(X))\n",
    "\n",
    "T.Resize(250)(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def change_color(train_data):\n",
    "    num, size = train_data.shape\n",
    "    output = np.zeros([num, size, 3], dtype=np.uint8)\n",
    "    color = np.zeros([num, 3], dtype=np.uint8)\n",
    "    for i in range(num):\n",
    "        print(i)\n",
    "        r = np.random.randint(0, 200)\n",
    "        g = np.random.randint(0, 200)\n",
    "        b = np.random.randint(0, 200)\n",
    "        color[i][0] = r\n",
    "        color[i][1] = g\n",
    "        color[i][2] = b\n",
    "        for j in range(size):\n",
    "            gray_scale = train_data[i][j]\n",
    "            if gray_scale!=0:\n",
    "                output[i][j][0] = r*gray_scale\n",
    "                output[i][j][1] = g*gray_scale\n",
    "                output[i][j][2] = b*gray_scale\n",
    "\n",
    "    return output, color"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
